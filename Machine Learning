Load Training Accuracy
If there is lot of misscalssification, model built is called Under Fitting
If there is moderate misscalssification, Just Fiting
If there is no miss classification and more accurate- Over Fitting

Convolutional Neural Networks - When we can not use regular neuron over more number of layers, we uses CNN
Training Dataset
Development Dataset
Test Dataset
Data points

Online Learning - If your training data poins are keeps changing after deployment in PROD, u can use online learning training technique to save datapoints earlier and train with new data contineusly

High Varience ->  Training Error 1% and Test Error - 11%
High Bias  -> Training error - 15% and test error  - 10%
High Bias & Varience -- Worst result -> -> Training error - 15% and test error  - 30%
Low Bias & varience  -- Best result  -- This should be the expected result. -> -> Training error - 0.5% and test error  - 1%

Cost Function: 

Suffling Distribution -> To make sure random data distribusion which is good to identify all data points

Regularization : Prevent from over fitting

L2 Regularization - usually uses this
L1 Regularization - only on future data points 
Implementing Regularization - Inverted dropout - 

Weight Matrix
Add little bit of error to smoothes the fitting
Regularization parameter
Elastic Net - mixing parameter
gradient descent formula - cost function
Slope -


Normalization - variance 1 weight 0
Linear activation function 
Softmax Regression - when we have more number of classification 


Vanishing Gradient 
Exploding Gradient



Xavier initialization
Sigmoid Function - Vanishing Gradient during back propagation

Gradient Checking - check back propagation is happening or not  


Hyper Parameters:



Hyper Parameters:

Vanishing Gradient 
Exploding Gradient



Xavier initialization
Sigmoid Function - Vanishing Gradient during back propagation


Gradient Checking - check back propagation is happening properly or not 
